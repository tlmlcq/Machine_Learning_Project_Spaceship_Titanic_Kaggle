{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Project - Spaceship Titanic Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from hyperopt import tpe\n",
    "from hyperopt import STATUS_OK\n",
    "from hyperopt import Trials\n",
    "from hyperopt import hp\n",
    "from hyperopt import fmin\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "\n",
    "df_train.drop(['Name'], axis=1, inplace=True)\n",
    "df_test.drop(['Name'], axis=1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAITEMENT DES DONNEES ET CREATION DE NOUVELLES VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df(df, test=False):\n",
    "    df = pd.get_dummies(df, columns=['HomePlanet', 'Destination'], dummy_na=True)\n",
    "    df['VIP']=df['VIP'].replace([False, True],[0,1])\n",
    "    df['CryoSleep']=df['CryoSleep'].replace([False, True],[0,1])\n",
    "    if(test==False):\n",
    "        df['Transported'] = df['Transported'].replace([False, True], [0,1])\n",
    "    df['Deck'] = np.nan\n",
    "    df['Side'] = np.nan\n",
    "    #1 hot encoding Deck & Side & filling with nas the na values from get_dummies()\n",
    "    for i in range(df.shape[0]):\n",
    "        #get the deck and sed\n",
    "        if type(df['Cabin'].iloc[i]) is str:\n",
    "            df['Deck'].iloc[i]=df['Cabin'].iloc[i][:1]\n",
    "            df['Side'].iloc[i]=df['Cabin'].iloc[i][-1:]\n",
    "        if(df['HomePlanet_nan'].iloc[i]==1):\n",
    "            #filling with nas where there should be nas but were replaced by get_dummies()\n",
    "            df['HomePlanet_Earth'].iloc[i] = np.nan\n",
    "            df['HomePlanet_Mars'].iloc[i] = np.nan\n",
    "            df['HomePlanet_Europa'].iloc[i] = np.nan\n",
    "        if(df['Destination_nan'].iloc[i]==1):\n",
    "            df['Destination_55 Cancri e'].iloc[i] = np.nan\n",
    "            df['Destination_PSO J318.5-22'].iloc[i] = np.nan\n",
    "            df['Destination_TRAPPIST-1e'].iloc[i] = np.nan\n",
    "    #1 hot encoding side\n",
    "    df['Side']=df['Side'].replace(['P', 'S'],[0,1])\n",
    "    #1 hot encoding Deck\n",
    "    res = pd.get_dummies(df, columns=['Deck'], dummy_na=True)\n",
    "    for i in range(res.shape[0]):\n",
    "        #filling with nas where there should be nas but were replaced by get_dummies()\n",
    "        if(res['Deck_nan'].iloc[i] == 1):\n",
    "            decks=['A', 'B', 'C', 'D', 'E', 'F', 'G', 'T']\n",
    "            for deck in decks:\n",
    "                res[f'Deck_{deck}'].iloc[i] = np.nan\n",
    "    res = res.drop(columns=['Cabin', 'Destination_nan', 'HomePlanet_nan', 'Deck_nan'])\n",
    "    \n",
    "    return res\n",
    "df_train = clean_df(df_train)\n",
    "df_test = clean_df(df_test, test=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GESTION DES VALEURS MANQUANTES (NA)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 - Remplir les valeurs manquantes avec la moyenne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handleNasMean(df):\n",
    "    colset = set(df.columns.to_list())\n",
    "    if('Transported' in colset):\n",
    "        imput = df.drop(columns=['Transported'])\n",
    "        colset.remove('Transported')\n",
    "    else:\n",
    "        imput = df\n",
    "\n",
    "    numericals = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa']\n",
    "    for num in numericals: # numericals are columns with numerical data, colset are thoses one hot encoded\n",
    "        colset.remove(num)\n",
    "    #for categorical data imputing with most frequent data\n",
    "    for col in colset:\n",
    "        imput[col] = imput[col].fillna(imput[col].mode()[0])\n",
    "    \n",
    "    #for numerical data imputing with mean\n",
    "    for col in numericals:\n",
    "        imput[col] = imput[col].fillna(imput[col].mean())\n",
    "    #adding new features\n",
    "    imput['tot_bill']=0\n",
    "    imput['avg_bill']=0\n",
    "    imput['max_bill']=0\n",
    "    imput['type_max_bill']='A'\n",
    "    imput['type_max_bill_RoomService']=0\n",
    "    imput['type_max_bill_FoodCourt']=0\n",
    "    imput['type_max_bill_ShoppingMall']=0\n",
    "    imput['type_max_bill_Spa']=0\n",
    "    imput['type_max_bill_VRDeck']=0\n",
    "\n",
    "    dic={0:'RoomService', 1:'FoodCourt', 2:'ShoppingMall', 3:'Spa',4:'VRDeck', np.nan:np.nan}\n",
    "    for i in range(imput.shape[0]):\n",
    "        imput['tot_bill'].iloc[i]=imput['RoomService'].iloc[i]+imput['FoodCourt'].iloc[i]+imput['ShoppingMall'].iloc[i]+imput['Spa'].iloc[i]+imput['VRDeck'].iloc[i]\n",
    "        imput['avg_bill'].iloc[i]=(imput['RoomService'].iloc[i]+imput['FoodCourt'].iloc[i]+imput['ShoppingMall'].iloc[i]+imput['Spa'].iloc[i]+imput['VRDeck'].iloc[i])/5\n",
    "        tab=[imput['RoomService'].iloc[i],imput['FoodCourt'].iloc[i],imput['ShoppingMall'].iloc[i],imput['Spa'].iloc[i], imput['VRDeck'].iloc[i]]\n",
    "        imput['max_bill'].iloc[i]=np.max(tab)\n",
    "        if math.isnan(np.max(tab)) is False:\n",
    "            imput['type_max_bill'].iloc[i]=dic[tab.index(np.max(tab))]\n",
    "        else:\n",
    "            imput['type_max_bill'].iloc[i]=np.nan\n",
    "    \n",
    "        if imput['type_max_bill'].iloc[i] == 'RoomService':\n",
    "            imput['type_max_bill_RoomService'].iloc[i] = 1\n",
    "        else:\n",
    "            imput['type_max_bill_RoomService'].iloc[i] = 0\n",
    "    \n",
    "        if imput['type_max_bill'].iloc[i] == 'FoodCourt':\n",
    "            imput['type_max_bill_FoodCourt'].iloc[i] = 1\n",
    "        else:\n",
    "            imput['type_max_bill_FoodCourt'].iloc[i] = 0\n",
    "        \n",
    "        if imput['type_max_bill'].iloc[i] == 'ShoppingMall':\n",
    "            imput['type_max_bill_ShoppingMall'].iloc[i] = 1\n",
    "        else:\n",
    "            imput['type_max_bill_ShoppingMall'].iloc[i] = 0\n",
    "\n",
    "        if imput['type_max_bill'].iloc[i] == 'Spa':\n",
    "            imput['type_max_bill_Spa'].iloc[i] = 1\n",
    "        else:\n",
    "            imput['type_max_bill_Spa'].iloc[i] = 0\n",
    "\n",
    "        if imput['type_max_bill'].iloc[i] == 'VRDeck':\n",
    "            imput['type_max_bill_Spa'].iloc[i] = 1\n",
    "        else:\n",
    "            imput['type_max_bill_VRDeck'].iloc[i] = 0\n",
    "    \n",
    "    imput.drop(columns=['type_max_bill'], inplace=True)\n",
    "\n",
    "    return imput\n",
    "#### 2 - Remplir les valeurs manquantes avec un algorithme KNN\n",
    "df_train_na_mean = handleNasMean(df_train)\n",
    "df_test_na_mean = handleNasMean(df_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 - Remplir les valeurs manquantes avec un algorithme KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handleNasKnn(df):\n",
    "    # scaler = StandardScaler()\n",
    "    # columnsToScale = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "    # df[columnsToScale] = scaler.fit_transform(df[columnsToScale])\n",
    "    #Replacing the nas (nas from categorical data has to be recategorized)\n",
    "    imputer = KNNImputer(n_neighbors=25)\n",
    "    colset = set(df.columns.to_list())\n",
    "    if('Transported' in colset):\n",
    "        imput = df.drop(columns=['Transported'])\n",
    "    else:\n",
    "        imput = df\n",
    "    imput = pd.DataFrame(imputer.fit_transform(imput), columns=imput.columns)\n",
    "    #adding new features\n",
    "    imput['tot_bill']=0\n",
    "    imput['avg_bill']=0\n",
    "    imput['max_bill']=0\n",
    "    imput['type_max_bill']='A'\n",
    "    imput['type_max_bill_RoomService']=0\n",
    "    imput['type_max_bill_FoodCourt']=0\n",
    "    imput['type_max_bill_ShoppingMall']=0\n",
    "    imput['type_max_bill_Spa']=0\n",
    "    imput['type_max_bill_VRDeck']=0\n",
    "\n",
    "    dic={0:'RoomService', 1:'FoodCourt', 2:'ShoppingMall', 3:'Spa',4:'VRDeck', np.nan:np.nan}\n",
    "    for i in range(imput.shape[0]):\n",
    "        imput['tot_bill'].iloc[i]=imput['RoomService'].iloc[i]+imput['FoodCourt'].iloc[i]+imput['ShoppingMall'].iloc[i]+imput['Spa'].iloc[i]+imput['VRDeck'].iloc[i]\n",
    "        imput['avg_bill'].iloc[i]=(imput['RoomService'].iloc[i]+imput['FoodCourt'].iloc[i]+imput['ShoppingMall'].iloc[i]+imput['Spa'].iloc[i]+imput['VRDeck'].iloc[i])/5\n",
    "        tab=[imput['RoomService'].iloc[i],imput['FoodCourt'].iloc[i],imput['ShoppingMall'].iloc[i],imput['Spa'].iloc[i], imput['VRDeck'].iloc[i]]\n",
    "        imput['max_bill'].iloc[i]=np.max(tab)\n",
    "        if math.isnan(np.max(tab)) is False:\n",
    "            imput['type_max_bill'].iloc[i]=dic[tab.index(np.max(tab))]\n",
    "        else:\n",
    "            imput['type_max_bill'].iloc[i]=np.nan\n",
    "    \n",
    "        if imput['type_max_bill'].iloc[i] == 'RoomService':\n",
    "            imput['type_max_bill_RoomService'].iloc[i] = 1\n",
    "        else:\n",
    "            imput['type_max_bill_RoomService'].iloc[i] = 0\n",
    "    \n",
    "        if imput['type_max_bill'].iloc[i] == 'FoodCourt':\n",
    "            imput['type_max_bill_FoodCourt'].iloc[i] = 1\n",
    "        else:\n",
    "            imput['type_max_bill_FoodCourt'].iloc[i] = 0\n",
    "        \n",
    "        if imput['type_max_bill'].iloc[i] == 'ShoppingMall':\n",
    "            imput['type_max_bill_ShoppingMall'].iloc[i] = 1\n",
    "        else:\n",
    "            imput['type_max_bill_ShoppingMall'].iloc[i] = 0\n",
    "\n",
    "        if imput['type_max_bill'].iloc[i] == 'Spa':\n",
    "            imput['type_max_bill_Spa'].iloc[i] = 1\n",
    "        else:\n",
    "            imput['type_max_bill_Spa'].iloc[i] = 0\n",
    "\n",
    "        if imput['type_max_bill'].iloc[i] == 'VRDeck':\n",
    "            imput['type_max_bill_Spa'].iloc[i] = 1\n",
    "        else:\n",
    "            imput['type_max_bill_VRDeck'].iloc[i] = 0\n",
    "    encodeds = ['CryoSleep', 'VIP', 'HomePlanet_Earth', 'HomePlanet_Earth', 'HomePlanet_Europa', \n",
    "        'Destination_PSO J318.5-22', 'Destination_TRAPPIST-1e', 'Deck_A', 'Deck_B', 'Deck_C', 'Deck_D', 'Deck_E', 'Deck_F', 'Deck_G', 'Side'\n",
    "        ]\n",
    "    #for categorical data, we re-categorzie from knn\n",
    "    for col in encodeds:\n",
    "        for i in range(imput.shape[0]):\n",
    "            if(imput[col].iloc[i] - 0.5 <0):\n",
    "                imput[col].iloc[i] = 0\n",
    "            else:\n",
    "                imput[col].iloc[i] = 1\n",
    "    imput.drop(columns=['type_max_bill'], inplace=True)\n",
    "    return imput\n",
    "\n",
    "df_test_na_knn = handleNasKnn(df_test)\n",
    "df_train_na_knn = handleNasKnn(df_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 - Remplir les valeurs manquantes avec un algorithme RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handleNasForest(df, test=False):\n",
    "    imputer = MissForest()\n",
    "    colset = set(df.columns.to_list())\n",
    "    if('Transported' in colset):\n",
    "        imput = df.drop(columns=['Transported'])\n",
    "    else:\n",
    "        imput = df    \n",
    "    imput = pd.DataFrame(imputer.fit_transform(imput), columns=imput.columns)\n",
    "    return imput"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C - MODELE DE MACHINE LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_na_mean = df_train_na_mean.drop(columns=['PassengerId'])\n",
    "Y_train_na_mean = df_train['Transported']\n",
    "X_test_na_mean = df_test_na_mean.drop(['PassengerId'], axis=1)\n",
    "\n",
    "X_train_na_knn = df_train_na_knn.drop(columns=['PassengerId'])\n",
    "Y_train_na_knn = df_train['Transported']\n",
    "X_test_na_knn = df_test_na_knn.drop(['PassengerId'], axis=1)\n",
    "\n",
    "#importer depuis un fichier csv\n",
    "X_train_na_rf = pd.read_csv('df_train_na_rf_no_index.csv').drop(columns=['PassengerId'])\n",
    "Y_train_na_rf = df_train['Transported']\n",
    "X_test_na_rf = pd.read_csv('df_test_na_rf_no_index.csv').drop(columns=['PassengerId'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 - Régression logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modele_logit = LogisticRegression(penalty='none',solver='newton-cg')\n",
    "\n",
    "#Test avec df_na_mean\n",
    "modele_logit.fit(X_train_na_mean,Y_train_na_mean)\n",
    "df_coeff_na_mean=pd.DataFrame(np.concatenate([modele_logit.intercept_.reshape(-1,1),\n",
    "                             modele_logit.coef_],axis=1),\n",
    "             index = [\"coef\"],\n",
    "             columns = [\"constante\"]+list(X_train_na_mean.columns)).T\n",
    "res_logit_na_mean = modele_logit.predict(X_test_na_mean)\n",
    "df_res_logit_na_mean=pd.DataFrame(df_test['PassengerId'])\n",
    "df_res_logit_na_mean['Transported']=res_logit_na_mean\n",
    "df_res_logit_na_mean['Transported']=df_res_logit_na_mean['Transported'].replace([0,1],[False, True])\n",
    "df_res_logit_na_mean.set_index('PassengerId').to_csv('test_prediction_logit_na_mean.csv')\n",
    "\n",
    "#Test avec df_na_knn\n",
    "modele_logit.fit(X_train_na_knn,Y_train_na_knn)\n",
    "df_coeff_na_knn=pd.DataFrame(np.concatenate([modele_logit.intercept_.reshape(-1,1),\n",
    "                             modele_logit.coef_],axis=1),\n",
    "             index = [\"coef\"],\n",
    "             columns = [\"constante\"]+list(X_train_na_knn.columns)).T\n",
    "res_logit_na_knn = modele_logit.predict(X_test_na_knn)\n",
    "df_res_logit_na_knn=pd.DataFrame(df_test['PassengerId'])\n",
    "df_res_logit_na_knn['Transported']=res_logit_na_knn\n",
    "df_res_logit_na_knn['Transported']=df_res_logit_na_knn['Transported'].replace([0,1],[False, True])\n",
    "df_res_logit_na_knn.set_index('PassengerId').to_csv('test_prediction_logit_na_knn.csv')\n",
    "\n",
    "#Test avec df_na_rf\n",
    "modele_logit.fit(X_train_na_rf,Y_train_na_rf)\n",
    "df_coeff_na_rf=pd.DataFrame(np.concatenate([modele_logit.intercept_.reshape(-1,1),\n",
    "                             modele_logit.coef_],axis=1),\n",
    "             index = [\"coef\"],\n",
    "             columns = [\"constante\"]+list(X_train_na_rf.columns)).T\n",
    "res_logit_na_rf = modele_logit.predict(X_test_na_rf)\n",
    "df_res_logit_na_rf=pd.DataFrame(df_test['PassengerId'])\n",
    "df_res_logit_na_rf['Transported']=res_logit_na_rf\n",
    "df_res_logit_na_rf['Transported']=df_res_logit_na_rf['Transported'].replace([0,1],[False, True])\n",
    "df_res_logit_na_rf.set_index('PassengerId').to_csv('test_prediction_logit_na_rf.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 - Modèle KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_knn_na_mean = KNeighborsClassifier()\n",
    "model_knn_na_knn = KNeighborsClassifier()\n",
    "model_knn_na_rf = KNeighborsClassifier()\n",
    "\n",
    "#Test avec df_na_mean\n",
    "model_knn_na_mean.fit(X_train_na_mean,Y_train_na_mean)\n",
    "res_knn_na_mean = model_knn_na_mean.predict(X_test_na_mean)\n",
    "df_res_knn_na_mean=pd.DataFrame(df_test['PassengerId'])\n",
    "df_res_knn_na_mean['Transported']=res_knn_na_mean\n",
    "df_res_knn_na_mean['Transported']=df_res_knn_na_mean['Transported'].replace([0,1],[False, True])\n",
    "df_res_knn_na_mean.set_index('PassengerId').to_csv('test_prediction_knn_na_mean.csv')\n",
    "\n",
    "#Test avec df_na_knn\n",
    "model_knn_na_knn.fit(X_train_na_knn,Y_train_na_knn)\n",
    "res_knn_na_knn = model_knn_na_knn.predict(X_test_na_knn)\n",
    "df_res_knn_na_knn=pd.DataFrame(df_test['PassengerId'])\n",
    "df_res_knn_na_knn['Transported']=res_knn_na_knn\n",
    "df_res_knn_na_knn['Transported']=df_res_knn_na_knn['Transported'].replace([0,1],[False, True])\n",
    "df_res_knn_na_knn.set_index('PassengerId').to_csv('test_prediction_knn_na_knn.csv')\n",
    "\n",
    "#Test avec df_na_rf\n",
    "model_knn_na_rf.fit(X_train_na_rf,Y_train_na_rf)\n",
    "res_knn_na_rf = model_knn_na_rf.predict(X_test_na_rf)\n",
    "df_res_knn_na_rf=pd.DataFrame(df_test['PassengerId'])\n",
    "df_res_knn_na_rf['Transported']=res_knn_na_rf\n",
    "df_res_knn_na_rf['Transported']=df_res_knn_na_rf['Transported'].replace([0,1],[False, True])\n",
    "df_res_knn_na_rf.set_index('PassengerId').to_csv('test_prediction_knn_na_rf.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 - Gradient Boosting XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb_na_mean = XGBClassifier()\n",
    "model_xgb_na_knn = XGBClassifier()\n",
    "model_xgb_na_rf = XGBClassifier()\n",
    "\n",
    "\n",
    "#Test avec df_na_mean\n",
    "model_xgb_na_mean.fit(X_train_na_mean,Y_train_na_mean)\n",
    "res_xgb_na_mean = model_xgb_na_mean.predict(X_test_na_mean)\n",
    "df_res_xgb_na_mean=pd.DataFrame(df_test['PassengerId'])\n",
    "df_res_xgb_na_mean['Transported']=res_xgb_na_mean\n",
    "df_res_xgb_na_mean['Transported']=df_res_xgb_na_mean['Transported'].replace([0,1],[False, True])\n",
    "df_res_xgb_na_mean.set_index('PassengerId').to_csv('test_prediction_xgb_na_mean.csv')\n",
    "\n",
    "#Test avec df_na_knn\n",
    "model_xgb_na_knn.fit(X_train_na_knn,Y_train_na_knn)\n",
    "res_xgb_na_knn = model_xgb_na_knn.predict(X_test_na_knn)\n",
    "df_res_xgb_na_knn=pd.DataFrame(df_test['PassengerId'])\n",
    "df_res_xgb_na_knn['Transported']=res_xgb_na_knn\n",
    "df_res_xgb_na_knn['Transported']=df_res_xgb_na_knn['Transported'].replace([0,1],[False, True])\n",
    "df_res_xgb_na_knn.set_index('PassengerId').to_csv('test_prediction_xgb_na_knn.csv')\n",
    "\n",
    "#Test avec df_na_rf\n",
    "model_xgb_na_rf.fit(X_train_na_rf,Y_train_na_rf)\n",
    "res_xgb_na_rf = model_xgb_na_rf.predict(X_test_na_rf)\n",
    "df_res_xgb_na_rf=pd.DataFrame(df_test['PassengerId'])\n",
    "df_res_xgb_na_rf['Transported']=res_xgb_na_rf\n",
    "df_res_xgb_na_rf['Transported']=df_res_xgb_na_rf['Transported'].replace([0,1],[False, True])\n",
    "df_res_xgb_na_rf.set_index('PassengerId').to_csv('test_prediction_xgb_na_rf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_imp_na_mean=pd.DataFrame()\n",
    "ft_imp_na_mean['Column_name']=X_train_na_mean.columns\n",
    "ft_imp_na_mean['value']=model_xgb_na_mean.feature_importances_\n",
    "ft_imp_na_mean.sort_values(by='value', ascending=False)\n",
    "\n",
    "ft_imp_na_knn=pd.DataFrame()\n",
    "ft_imp_na_knn['Column_name']=X_train_na_knn.columns\n",
    "ft_imp_na_knn['value']=model_xgb_na_knn.feature_importances_\n",
    "ft_imp_na_knn.sort_values(by='value', ascending=False)\n",
    "\n",
    "ft_imp_na_rf=pd.DataFrame()\n",
    "ft_imp_na_rf['Column_name']=X_train_na_rf.columns\n",
    "ft_imp_na_rf['value']=model_xgb_na_rf.feature_importances_\n",
    "ft_imp_na_rf.sort_values(by='value', ascending=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 - Modèle CatBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cbc_na_mean = CatBoostClassifier(verbose=False)\n",
    "model_cbc_na_knn = CatBoostClassifier(verbose=False)\n",
    "model_cbc_na_rf = CatBoostClassifier(verbose=False)\n",
    "\n",
    "\n",
    "#Test avec df_na_mean\n",
    "model_cbc_na_mean.fit(X_train_na_mean,Y_train_na_mean)\n",
    "res_cbc_na_mean = model_cbc_na_mean.predict(X_test_na_mean)\n",
    "df_res_cbc_na_mean=pd.DataFrame(df_test['PassengerId'])\n",
    "df_res_cbc_na_mean['Transported']=res_cbc_na_mean\n",
    "df_res_cbc_na_mean['Transported']=df_res_cbc_na_mean['Transported'].replace([0,1],[False, True])\n",
    "df_res_cbc_na_mean.set_index('PassengerId').to_csv('test_prediction_cbc_na_mean.csv')\n",
    "\n",
    "#Test avec df_na_knn\n",
    "model_cbc_na_knn.fit(X_train_na_knn,Y_train_na_knn)\n",
    "res_cbc_na_knn = model_cbc_na_knn.predict(X_test_na_knn)\n",
    "df_res_cbc_na_knn=pd.DataFrame(df_test['PassengerId'])\n",
    "df_res_cbc_na_knn['Transported']=res_cbc_na_knn\n",
    "df_res_cbc_na_knn['Transported']=df_res_cbc_na_knn['Transported'].replace([0,1],[False, True])\n",
    "df_res_cbc_na_knn.set_index('PassengerId').to_csv('test_prediction_cbc_na_knn.csv')\n",
    "\n",
    "#Test avec df_na_rf\n",
    "model_cbc_na_rf.fit(X_train_na_rf,Y_train_na_rf)\n",
    "res_cbc_na_rf = model_cbc_na_rf.predict(X_test_na_rf)\n",
    "df_res_cbc_na_rf=pd.DataFrame(df_test['PassengerId'])\n",
    "df_res_cbc_na_rf['Transported']=res_cbc_na_rf\n",
    "df_res_cbc_na_rf['Transported']=df_res_cbc_na_rf['Transported'].replace([0,1],[False, True])\n",
    "df_res_cbc_na_rf.set_index('PassengerId').to_csv('test_prediction_cbc_na_rf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_imp_na_mean=pd.DataFrame()\n",
    "ft_imp_na_mean['Column_name']=X_train_na_mean.columns\n",
    "ft_imp_na_mean['value']=model_cbc_na_mean.feature_importances_\n",
    "ft_imp_na_mean.sort_values(by='value', ascending=False)\n",
    "\n",
    "ft_imp_na_knn=pd.DataFrame()\n",
    "ft_imp_na_knn['Column_name']=X_train_na_knn.columns\n",
    "ft_imp_na_knn['value']=model_cbc_na_knn.feature_importances_\n",
    "ft_imp_na_knn.sort_values(by='value', ascending=False)\n",
    "\n",
    "ft_imp_na_rf=pd.DataFrame()\n",
    "ft_imp_na_rf['Column_name']=X_train_na_rf.columns\n",
    "ft_imp_na_rf['value']=model_cbc_na_rf.feature_importances_\n",
    "ft_imp_na_rf.sort_values(by='value', ascending=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D - OPTIMISATION DES HYPERPARAMETRES"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 - Bayesian Search - essai non concluant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [14:45<00:00, 44.29s/trial, best loss: 0.1875]            \n",
      "0.1875\n",
      "Etape 1 : SCORE de 0.1875\n"
     ]
    }
   ],
   "source": [
    "MAX_EVALS = 100\n",
    "df_X = X_train_na_rf\n",
    "df_Y = Y_train_na_rf\n",
    "\n",
    "def objective(params):\n",
    "    X_train,X_test, y_train, y_test = train_test_split(df_X, df_Y, test_size=0.3)\n",
    "    model = CatBoostClassifier(depth=int(params['depth']), iterations=int(params['iterations']),\n",
    "                             learning_rate=params['learning_rate'],\n",
    "                             l2_leaf_reg=int(params['l2_leaf_reg']), verbose=False)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    actual = np.array(y_test)\n",
    "    forecast = np.array(preds)\n",
    "    best_score = accuracy_score(actual, forecast)\n",
    "    return (1-best_score)\n",
    "\n",
    "space_param = {\n",
    "    \"depth\": hp.choice(\"depth\", np.arange(5,12, dtype=int)),\n",
    "    \"learning_rate\": hp.uniform(\"learning_rate\", 0,1),\n",
    "    \"iterations\": hp.choice(\"iterations\", np.arange(1000,5000,100)),\n",
    "    \"l2_leaf_reg\": hp.choice(\"l2_leaf_reg\", np.arange(10,1000,100)),\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def bayesian_opti(objective, space_param, MAX_EVALS=100):\n",
    "    # Algorithm\n",
    "    tpe_algorithm = tpe.suggest\n",
    "\n",
    "    # Trials object to track progress\n",
    "    bayes_trials = Trials()\n",
    "\n",
    "    # Optimize\n",
    "    best = fmin(fn=objective, space=space_param, algo=tpe_algorithm, max_evals=MAX_EVALS, trials=bayes_trials,\n",
    "                return_argmin=False)\n",
    "    df_best = pd.DataFrame.from_dict(best, orient='index')\n",
    "    print(bayes_trials.best_trial['result']['loss'])\n",
    "    return df_best, bayes_trials.best_trial['result']['loss']\n",
    "\n",
    "def multi_bay_opti(objective, space_param, nb=10):\n",
    "    final = pd.DataFrame(\n",
    "        columns=['depth', 'iterations', 'learning_rate', 'l2_leaf_reg','SCORE'], index=range(nb))\n",
    "    for i in range(nb):\n",
    "        optim = bayesian_opti(objective, space_param, 20)\n",
    "        final['depth'].iloc[i] = optim[0].loc['depth'][0]\n",
    "        final['iterations'].iloc[i] = optim[0].loc['iterations'][0]\n",
    "        final['learning_rate'].iloc[i] = optim[0].loc['learning_rate'][0]\n",
    "        final['l2_leaf_reg'].iloc[i] = optim[0].loc['l2_leaf_reg'][0]\n",
    "        final['SCORE'].iloc[i] = optim[1]\n",
    "        print('Etape {} : SCORE de {}'.format(i + 1, optim[1]))\n",
    "    final.to_csv('res_bayes_multisearch_hyperparam.csv')\n",
    "    return final\n",
    "\n",
    "final = multi_bay_opti(objective, space_param, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 - Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model=CatBoostClassifier(verbose=False)\n",
    "grid = {\"depth\": [6,8,10,11],\n",
    "    \"learning_rate\": [0.001,0.01,0.1],\n",
    "    \"iterations\": [1600,2500,4000]}\n",
    "\n",
    "grid_srch_cat = GridSearchCV(estimator=model, param_grid=grid, cv=3)\n",
    "grid_srch_cat.fit(X_train_na_rf, Y_train_na_rf)\n",
    "params = grid_srch_cat.best_params_\n",
    "print(\"The best estimator across ALL searched params:\",grid_srch_cat.best_estimator_)\n",
    "print(\"The best score across ALL searched params:\",grid_srch_cat.best_score_)\n",
    "print(\"The best parameters across ALL searched params:\",grid_srch_cat.best_params_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DERNIERE PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cbc_na_mean = CatBoostClassifier(depth= 8,\n",
    "        iterations= 3300,\n",
    "        learning_rate= 0.1,\n",
    "        l2_leaf_reg= 800,\n",
    "        verbose=False)\n",
    "model_cbc_na_mean.fit(X_train_na_rf, Y_train_na_rf)\n",
    "res_cbc_na_rf = model_cbc_na_mean.predict(X_test_na_rf)\n",
    "df_res_cbc_na_rf=pd.DataFrame(df_test['PassengerId'])\n",
    "df_res_cbc_na_rf['Transported']=res_cbc_na_rf\n",
    "df_res_cbc_na_rf['Transported']=df_res_cbc_na_rf['Transported'].replace([0,1],[False, True])\n",
    "df_res_cbc_na_rf.set_index('PassengerId').to_csv('LAST_prediction_cbc_na_rf.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
